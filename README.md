# Generative Models of Visual Perception: VAE & ß-VAE on Caltech-101

This notebook is a step-by-step exploration of how **Variational Autoencoders (VAE)** and **ß-VAEs** can learn disentangled visual representations from natural images. It includes the full pipeline: data loading, model architecture, training, latent space analysis, and visual interpretation all on the **Caltech-101** dataset.

I recommend downloading and running the `.ipynb` file locally for best performance.

This project was developed as part of my coursework in **Cognition and Computation** at the University of Padova, combining deep learning with theories of visual neuroscience like **predictive coding** and **Bayesian inference**.

---

## Dataset

- **Caltech-101** – 101 object categories, ~50 images each  
- Format: RGB images resized to 64×64  
- Source DOI: [10.22002/D1.20086](https://doi.org/10.22002/D1.20086)  
- License: Public domain  

---

## Core Ideas

- Learn compressed, meaningful visual features using VAEs  
- Use ß-VAE to encourage disentanglement in the latent space  
- Analyze robustness, noise tolerance, and few-shot generalization  

---

## Contact

Feel free to reach out for collaborations, feedback, or just to geek out about neuroscience and neuroimaging!

**Oğulcan ULU**  
Padova, Italy  
ogulcan.ulu7@gmail.com | ogulcan.ulu@studenti.unipd.it
